{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe089f7b-5196-4d74-a4d0-934340cc468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345e2b55-9368-4cf3-aab9-78a4d5f3313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 for RandomForest and AdaBoost: Read the dataset and do necessary preprocessing\n",
    "df = pd.read_csv(\"IRIS.csv\")\n",
    "\n",
    "# Data imputation for missing values\n",
    "# Assuming numerical features; for categorical features, strategy might be 'most_frequent'\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df[df.select_dtypes(include=[np.number]).columns] = imputer.fit_transform(df.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Encoding categorical to numerical if necessary\n",
    "# Assuming 'species' is a categorical column that needs to be encoded\n",
    "if df['species'].dtype == 'object':\n",
    "    encoder = LabelEncoder()\n",
    "    df['species'] = encoder.fit_transform(df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66190fe-9ca6-494a-8aeb-4a5ec3c11a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 for RandomForest and AdaBoost: Choose independent (X) and dependent (Y) variables\n",
    "X = df.drop('species', axis=1)  # Assuming 'species' is the target variable\n",
    "Y = df['species']\n",
    "\n",
    "# Splitting dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f050cff-6101-497c-8662-a57e25655d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Parameters: {'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n",
      "Random Forest Parameters: {'n_estimators': 150, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n",
      "Random Forest Parameters: {'n_estimators': 200, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n",
      "Random Forest Parameters: {'n_estimators': 100, 'criterion': 'entropy', 'max_depth': None, 'max_features': None}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3 & Q4 & Q5 for RandomForest: Creating models with different parameters, finding best parameters, and evaluating\n",
    "rf_params = [\n",
    "    {'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto'},\n",
    "    {'n_estimators': 150, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt'},\n",
    "    {'n_estimators': 200, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2'},\n",
    "    {'n_estimators': 100, 'criterion': 'entropy', 'max_depth': None, 'max_features': None},\n",
    "]\n",
    "\n",
    "for params in rf_params:\n",
    "    clf_rf = RandomForestClassifier(**params, random_state=0)\n",
    "    clf_rf.fit(X_train, Y_train)\n",
    "    Y_pred = clf_rf.predict(X_test)\n",
    "    print(\"Random Forest Parameters:\", params)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, Y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "    print(\"Precision:\", precision_score(Y_test, Y_pred, average='macro'))\n",
    "    print(\"Recall:\", recall_score(Y_test, Y_pred, average='macro'))\n",
    "    print(\"F1 Score:\", f1_score(Y_test, Y_pred, average='macro'))\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    print(\"----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4eeff23-ee4d-4853-a83c-ae2e5e48a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Parameters: {'n_estimators': 50, 'learning_rate': 1.0, 'algorithm': 'SAMME.R'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n",
      "AdaBoost Parameters: {'n_estimators': 100, 'learning_rate': 0.5, 'algorithm': 'SAMME'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n",
      "AdaBoost Parameters: {'n_estimators': 150, 'learning_rate': 0.75, 'algorithm': 'SAMME.R'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n",
      "AdaBoost Parameters: {'n_estimators': 200, 'learning_rate': 1.0, 'algorithm': 'SAMME'}\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3 & Q4 & Q5 for AdaBoost: Creating models with different parameters, finding best parameters, and evaluating\n",
    "ab_params = [\n",
    "    {'n_estimators': 50, 'learning_rate': 1.0, 'algorithm': 'SAMME.R'},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.5, 'algorithm': 'SAMME'},\n",
    "    {'n_estimators': 150, 'learning_rate': 0.75, 'algorithm': 'SAMME.R'},\n",
    "    {'n_estimators': 200, 'learning_rate': 1.0, 'algorithm': 'SAMME'},\n",
    "]\n",
    "\n",
    "for params in ab_params:\n",
    "    clf_ab = AdaBoostClassifier(**params, random_state=0)\n",
    "    clf_ab.fit(X_train, Y_train)\n",
    "    Y_pred = clf_ab.predict(X_test)\n",
    "    print(\"AdaBoost Parameters:\", params)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, Y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "    print(\"Precision:\", precision_score(Y_test, Y_pred, average='macro'))\n",
    "    print(\"Recall:\", recall_score(Y_test, Y_pred, average='macro'))\n",
    "    print(\"F1 Score:\", f1_score(Y_test, Y_pred, average='macro'))\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    print(\"----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75ff06eb-992f-4f32-b166-98610610d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FCM in module fcmeans.main:\n",
      "\n",
      "class FCM(pydantic.main.BaseModel)\n",
      " |  FCM(*, n_clusters: typing.Annotated[int, Ge(ge=1)] = 5, max_iter: typing.Annotated[int, Ge(ge=1), Le(le=1000)] = 150, m: typing.Annotated[float, Ge(ge=1.0)] = 2.0, error: typing.Annotated[float, Ge(ge=1e-09)] = 1e-05, random_state: Optional[int] = None, trained: bool = False, n_jobs: typing.Annotated[int, Ge(ge=1)] = 1, verbose: Optional[bool] = False, distance: Union[fcmeans.main.DistanceOptions, Callable, NoneType] = <DistanceOptions.euclidean: 'euclidean'>, distance_params: Optional[Dict] = {}, **extra_data: Any) -> None\n",
      " |  \n",
      " |  Fuzzy C-means Model\n",
      " |  \n",
      " |  Attributes:\n",
      " |      n_clusters (int): The number of clusters to form as well as the number\n",
      " |      of centroids to generate by the fuzzy C-means.\n",
      " |      max_iter (int): Maximum number of iterations of the fuzzy C-means\n",
      " |      algorithm for a single run.\n",
      " |      m (float): Degree of fuzziness: $m \\in (1, \\infty)$.\n",
      " |      error (float): Relative tolerance with regards to Frobenius norm of\n",
      " |      the difference\n",
      " |      in the cluster centers of two consecutive iterations to declare\n",
      " |      convergence.\n",
      " |      random_state (Optional[int]): Determines random number generation for\n",
      " |      centroid initialization.\n",
      " |      Use an int to make the randomness deterministic.\n",
      " |      trained (bool): Variable to store whether or not the model has been\n",
      " |      trained.\n",
      " |  \n",
      " |  Returns:\n",
      " |      FCM: A FCM model.\n",
      " |  \n",
      " |  Raises:\n",
      " |      ReferenceError: If called without the model being trained\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FCM\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X: numpy.ndarray[typing.Any, numpy.dtype[+_ScalarType_co]]) -> None\n",
      " |      Train the fuzzy-c-means model\n",
      " |      \n",
      " |      Args:\n",
      " |          X (NDArray): Training instances to cluster.\n",
      " |  \n",
      " |  predict(self, X: numpy.ndarray[typing.Any, numpy.dtype[+_ScalarType_co]]) -> numpy.ndarray[typing.Any, numpy.dtype[+_ScalarType_co]]\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      Args:\n",
      " |          X (NDArray): New data to predict.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ReferenceError: If it called without the model being trained.\n",
      " |      \n",
      " |      Returns:\n",
      " |          NDArray: Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  soft_predict(self, X: numpy.ndarray[typing.Any, numpy.dtype[+_ScalarType_co]]) -> numpy.ndarray[typing.Any, numpy.dtype[+_ScalarType_co]]\n",
      " |      Soft predict of FCM\n",
      " |      \n",
      " |      Args:\n",
      " |          X (NDArray): New data to predict.\n",
      " |      \n",
      " |      Returns:\n",
      " |          NDArray: Fuzzy partition array, returned as an array with\n",
      " |          n_samples rows and n_clusters columns.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  centers\n",
      " |  \n",
      " |  partition_coefficient\n",
      " |      Partition coefficient\n",
      " |      \n",
      " |      Equation 12a of\n",
      " |      [this paper](https://doi.org/10.1016/0098-3004(84)90020-7).\n",
      " |  \n",
      " |  partition_entropy_coefficient\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'distance': typing.Union[fcmeans.main.DistanceOptio...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'cls': <class 'fcmeans.main.FCM'>, 'config...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = False\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = {'BaseModel': <pydantic._internal._mod...\n",
      " |  \n",
      " |  __pydantic_post_init__ = None\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"FCM\", validator=Model(...\n",
      " |  \n",
      " |  __signature__ = <Signature (*, n_clusters: typing.Annotated[int,... Op...\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'allow'}\n",
      " |  \n",
      " |  model_fields = {'distance': FieldInfo(annotation=Union[DistanceOptions...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self: 'Model') -> 'Model'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self: 'Model', memo: 'dict[int, Any] | None' = None) -> 'Model'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |      \n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'typing.Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Model'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```py\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  dict(self, *, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'typing.Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'typing.Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self: 'Model', *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Model'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.7/concepts/serialization/#model_copy\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"typing_extensions.Literal['json', 'python'] | str\" = 'python', include: 'IncEx' = None, exclude: 'IncEx' = None, context: 'dict[str, Any] | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.7/concepts/serialization/#modelmodel_dump\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx' = None, exclude: 'IncEx' = None, context: 'dict[str, Any] | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.7/concepts/serialization/#modelmodel_dump_json\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |      \n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: The set of field names accepted for the Model instance.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'dict[str, Any] | None' = None) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'dict[str, Any] | None' = None) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Usage docs: https://docs.pydantic.dev/2.7/concepts/json/#json-parsing\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If `json_data` is not a JSON string.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'dict[str, Any] | None' = None) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate the given object contains string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object contains string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'typing.Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fcmeans\n",
    "help(fcmeans.main.FCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a7326-797d-4896-81ec-59e52bdfe947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
